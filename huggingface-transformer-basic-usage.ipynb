{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2021-12-10T06:44:09.110980Z",
     "iopub.status.busy": "2021-12-10T06:44:09.110591Z",
     "iopub.status.idle": "2021-12-10T06:44:16.573591Z",
     "shell.execute_reply": "2021-12-10T06:44:16.572477Z",
     "shell.execute_reply.started": "2021-12-10T06:44:09.110937Z"
    },
    "id": "BHdOe9e80yBM",
    "outputId": "a2c616bc-02f9-4e6f-c217-ecf011a9fdc4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/ubuntu/.local/lib/python3.8/site-packages (4.9.2)\n",
      "Requirement already satisfied: huggingface-hub==0.0.12 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (0.0.12)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (4.62.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (2021.8.3)\n",
      "Requirement already satisfied: requests in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: sacremoses in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: typing-extensions in /home/ubuntu/.local/lib/python3.8/site-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ubuntu/.local/lib/python3.8/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers) (1.25.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: joblib in /home/ubuntu/.local/lib/python3.8/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: click in /home/ubuntu/.local/lib/python3.8/site-packages (from sacremoses->transformers) (8.0.1)\n",
      "Requirement already satisfied: six in /home/ubuntu/.local/lib/python3.8/site-packages (from sacremoses->transformers) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting started on a task with aÂ pipeline\n",
    "\n",
    "The easiest way to use a pre-trained model on a given task is to use pipeline(). ðŸ¤— Transformers provides the following tasks out of the box:\n",
    "Sentiment analysis: is a text positive or negative?\n",
    "\n",
    "1. Text generation (in English): provide a prompt and the model will generate what follows.\n",
    "2. Name entity recognition (NER): in an input sentence, label each word with the entity it represents (person, place, etc.)\n",
    "3. Question answering: provide the model with some context and a question, extract the answer from the context.\n",
    "4. Filling masked text: given a text with masked words (e.g., replaced by [MASK]), fill the blanks.\n",
    "5. Summarization: generate a summary of a long text.\n",
    "6. Language Translation: translate a text into another language.\n",
    "7. Feature extraction: return a tensor representation of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dnUOmbcB25Ua"
   },
   "source": [
    "### GPT2\n",
    "\n",
    "#### Model description\n",
    "\n",
    "**GPT-2** is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was trained to guess the next word in sentences.\n",
    "\n",
    "More precisely, inputs are sequences of continuous text of a certain length and the targets are the same sequence, shifted one token (word or piece of word) to the right. The model uses internally a mask-mechanism to make sure the predictions for the token i only uses the inputs from 1 to i but not the future tokens.\n",
    "\n",
    "This way, the model learns an inner representation of the English language that can then be used to extract features useful for downstream tasks. The model is best at what it was pretrained for however, which is generating texts from a prompt.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0pqplEwT5pKa"
   },
   "source": [
    "### Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fRs4XPDj1Zcj"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5UAKs6Ph2fUF",
    "outputId": "93e4804a-8b84-4453-ddb4-f893f916f998"
   },
   "outputs": [],
   "source": [
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "generator(\"Hello, I like to play cricket,\", max_length=60, num_return_sequences=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GhMVJMj-2o5S",
    "outputId": "1d69aab4-ca5c-4686-cbf5-9e961d4a1113"
   },
   "outputs": [],
   "source": [
    "generator(\"The Indian man worked as a\", max_length=10, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Z1iV8_7gkd2",
    "outputId": "b68768da-77a2-443d-b492-ec222cd81f3d"
   },
   "outputs": [],
   "source": [
    "generator(\"Machine learning is evolving technology\", max_length=10, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_Gm34tu5H88"
   },
   "source": [
    "### Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wr9r5V863pd_",
    "outputId": "d3257468-ae84-4e74-b4c8-c749779a86b1"
   },
   "outputs": [],
   "source": [
    "# Allocate a pipeline for sentiment-analysis\n",
    "#classifier = pipeline('sentiment-analysis')\n",
    "classifier('The secret of getting ahead is getting started.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6Mp0GYOgwVw"
   },
   "source": [
    "### Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bBBZQs4P5Cgu",
    "outputId": "7d0075e4-b8ab-45f6-e67c-6927d89ff154"
   },
   "outputs": [],
   "source": [
    "# Allocate a pipeline for question-answering\n",
    "question_answerer = pipeline('question-answering')\n",
    "question_answerer({\n",
    "    'question': 'What is the Newtons third law of motion?',\n",
    "    'context': 'Newtonâ€™s third law of motion states that, \"For every action there is equal and opposite reaction\"'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = pipeline(\"question-answering\")\n",
    "\n",
    "context = r\"\"\"\n",
    "Micorsoft was founded by Bill gates and Paul allen in the year 1975.\n",
    "The property of being prime (or not) is called primality.\n",
    "A simple but slow method of verifying the primality of a given number n is known as trial division.\n",
    "It consists of testing whether n is a multiple of any integer between 2 and itself.\n",
    "Algorithms much more efficient than trial division have been devised to test the primality of large numbers.\n",
    "These include the Millerâ€“Rabin primality test, which is fast but has a small probability of error, and the AKS primality test, which always produces the correct answer in polynomial time but is too slow to be practical.\n",
    "Particularly fast methods are available for numbers of special forms, such as Mersenne numbers.\n",
    "As of January 2016, the largest known prime number has 22,338,618 decimal digits.\n",
    "\"\"\"\n",
    "\n",
    "#Question 1\n",
    "result = nlp(question=\"What is a simple method to verify primality?\", context=context)\n",
    "\n",
    "print(f\"Answer 1: '{result['answer']}'\")\n",
    "\n",
    "#Question 2\n",
    "result = nlp(question=\"When did Bill gates founded Microsoft?\", context=context)\n",
    "\n",
    "print(f\"Answer 2: '{result['answer']}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT\n",
    "\n",
    "The BERT model was proposed in BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. Itâ€™s a bidirectional transformer pretrained using a combination of masked language modeling objective and next sentence prediction on a large corpus comprising the Toronto Book Corpus and Wikipedia.\n",
    "\n",
    "The abstract from the paper is the following:\n",
    "\n",
    "> We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\n",
    "\n",
    "BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhc-2_m86GbI"
   },
   "source": [
    "### Text prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CajALD2X5SoX",
    "outputId": "44a08518-91e0-4d69-ee37-4049f35def4e"
   },
   "outputs": [],
   "source": [
    "unmasker = pipeline('fill-mask', model='bert-base-cased')\n",
    "unmasker(\"Hello, My name is [MASK].\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jyRaHQj-FhVc"
   },
   "source": [
    "### Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T06:44:16.575648Z",
     "iopub.status.busy": "2021-12-10T06:44:16.575292Z",
     "iopub.status.idle": "2021-12-10T06:44:16.581200Z",
     "shell.execute_reply": "2021-12-10T06:44:16.580255Z",
     "shell.execute_reply.started": "2021-12-10T06:44:16.575613Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTICLE = \"\"\"Email classification using bert embeddings and classifier, also classify the emails \n",
    "using svm, logistic regression, decision tree. Email classification using LSTM model, \n",
    "tuning the hyperparameters of the model, also implemented several other scripts on email \n",
    "classification dataset (dbscan.py, lda.py). Model metrics save completed, vm environment setup\n",
    "will continue tomorrow as more problems occur in vm, output metrics of bert and lstm \n",
    "are having problem because they are not 1d arrays, looking for workaround. Making Algorithms \n",
    "presentations and waiting for Akash to give more autoML algorithms to implement.\n",
    "Implementing Pytorch Sentiment Analysis, 2/6 done. Implementing Pytorch Sentiment Analysis, done 4 \n",
    "algorithms out of 6.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTICLE = \"\"\"Worked on predicting credit risk modelling and doing exploratory data analysis on its dataset. \n",
    " Worked on Insurance Claim Prediction, predicting the medical cost billed by medical insurance, a regression problem.\n",
    " Implemented Health Insurance Claim Prediction, predicting whether a person will take up health insurance bu seeing his other data of previous insurance and demographics.\n",
    " Worked on Reamaining Useful Life Prediction on NASA Jet Engine Dataset, prediction the useful life remaining of Jet Engines.\n",
    " Worked on Car Insurance Claim Dataset to generate some insights about car insurance claims and see what factors will make customers more likely to be repeat offenders.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T06:47:25.600041Z",
     "iopub.status.busy": "2021-12-10T06:47:25.599650Z",
     "iopub.status.idle": "2021-12-10T06:47:49.579212Z",
     "shell.execute_reply": "2021-12-10T06:47:49.578271Z",
     "shell.execute_reply.started": "2021-12-10T06:47:25.600005Z"
    },
    "id": "rNvs2m3wFDTE",
    "outputId": "5c3bca8a-9fee-4d9c-cb16-972a5f4c6719"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Email classification using bert embeddings and classifier, also classify the emails  using svm, logistic regression, decision tree.py . Email classification  using LSTM model, tuning the hyperparameters of the model, also implemented several other scripts on email classification dataset (dbscan.py,\n"
     ]
    }
   ],
   "source": [
    "#Summarization is currently supported by Bart and T5.\n",
    "\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "summary=summarizer(ARTICLE, max_length=70, min_length=30, do_sample=False)[0]\n",
    "\n",
    "print(summary['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXbYuhuDF2oH"
   },
   "source": [
    "### English to German translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mO3fz6_-FQhg",
    "outputId": "b65e1ee1-eee8-4978-bc8f-fd701147a1dc"
   },
   "outputs": [],
   "source": [
    "# English to German\n",
    "translator_ger = pipeline(\"translation_en_to_de\")\n",
    "print(\"German: \",translator_ger(\"Joe Biden became the 46th president of U.S.A.\", max_length=40)[0]['translation_text'])\n",
    "\n",
    "# English to French\n",
    "translator_fr = pipeline('translation_en_to_fr')\n",
    "print(\"French: \",translator_fr(\"Joe Biden became the 46th president of U.S.A\",  max_length=40)[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZH4RKwJJjb_"
   },
   "source": [
    "### Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h_xwPmfOF1WG",
    "outputId": "60ca3783-6534-486a-c35b-b2d8108605e1"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "\n",
    "# Let's chat for 5 lines\n",
    "for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwgWM6Q1vJ2C"
   },
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oImVDicAI9OM",
    "outputId": "eed02ecd-2b96-41b9-f152-933193452bbe"
   },
   "outputs": [],
   "source": [
    "nlp_token_class = pipeline('ner') \n",
    "nlp_token_class('Ronaldo was born in 1985, he plays for Juventus and Portugal. ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot Learning\n",
    "Zero-Shot learning method aims to solve a task without receiving any example of that task at training phase. The task of recognizing an object from a given image where there weren't any example images of that object during training phase can be considered as an example of Zero-Shot Learning task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import pipeline, set_seed\n",
    "\n",
    "classifier_zsl = pipeline(\"zero-shot-classification\")\n",
    "\n",
    "sequence_to_classify = \"Bill gates founded a company called Microsoft in the year 1975\"\n",
    "candidate_labels = [\"Europe\", \"Sports\",'Leadership','business', \"politics\",\"startup\"]\n",
    "classifier_zsl(sequence_to_classify, candidate_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "nlp_features = pipeline('feature-extraction')\n",
    "output = nlp_features('Deep learning is a branch of Machine learning')\n",
    "np.array(output).shape   # output: (Samples, Tokens, Vector Size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using transformers in Widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "nlp_qaA = pipeline('question-answering')\n",
    "\n",
    "context = widgets.Textarea(\n",
    "    value='Einstein is famous for the general theory of relativity',\n",
    "    placeholder='Enter something',\n",
    "    description='Context:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "query = widgets.Text(\n",
    "    value='Why is Einstein famous for ?',\n",
    "    placeholder='Enter something',\n",
    "    description='Question:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "def forward(_):\n",
    "    if len(context.value) > 0 and len(query.value) > 0: \n",
    "        output = nlp_qaA(question=query.value, context=context.value)            \n",
    "        print(output)\n",
    "\n",
    "query.on_submit(forward)\n",
    "display(context, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying text with DistilBERT and Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import activations, optimizers, losses\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem statement\n",
    "\n",
    "Lets consider a small corpus of 10 Yelp reviews: 5 positive (class 1) and 5 negative (class 0). BERT (and its variants like DistilBERT) can be a great tool to use when you have a shortage of training data. that said, don't expect great results with just 10 reviews! Interchanging x and y with your own dataset is recommended ðŸ™‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks:\n",
    "\n",
    "1. Preprocessing the data\n",
    "2. Fine-tuning the model\n",
    "3. Testing the model\n",
    "4. Using the fine-tuned model to predict new samples\n",
    "5. Saving and loading the model for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " x = [\n",
    "     'Great customer service! The food was delicious! Definitely a come again.',\n",
    "     'The VEGAN options are super fire!!! And the plates come in big portions. Very pleased with this spot, I\\'ll definitely be ordering again',\n",
    "     'Come on, this place is family owned and operated, they are super friendly, the tacos are bomb.',\n",
    "     'This is such a great restaurant. Multiple times during days that we don\\'t want to cook, we\\'ve done takeout here and it\\'s been amazing. It\\'s fast and delicious.',\n",
    "     'Staff is really nice. Food is way better than average. Good cost benefit.',\n",
    "     'pricing for this, while relatively inexpensive for a Las Vegas attraction, is completely over the top.',\n",
    "     'At such a *fine* institution, I find the lack of knowledge and respect for the art appalling',\n",
    "     'If I could give one star I would...I walked out before my food arrived the customer service was horrible!',\n",
    "     'Wow the slowest drive thru I\\'ve ever been at WOWWWW. Horrible I won\\'t be coming back here ever again',\n",
    "     'Service: 1 out of 5 stars. They will mess up your order, not have it ready after 30 mins calling them before. Worst ran family business Ive ever seen.'\n",
    "]\n",
    "\n",
    "y = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "MAX_LEN = 20\n",
    "\n",
    "review = x[0]\n",
    "\n",
    "tkzr = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "inputs = tkzr(review, max_length=MAX_LEN, truncation=True, padding=True)\n",
    "\n",
    "print(f'review: \\'{review}\\'')\n",
    "print(f'input ids: {inputs[\"input_ids\"]}')\n",
    "print(f'attention mask: {inputs[\"attention_mask\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply this transformation to each review in our corpus. To do this we define a function construct_encodings, which maps the tokenizer to each review and aggregates them in encodings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_encodings(x, tkzr, max_len, trucation=True, padding=True):\n",
    "    return tkzr(x, max_length=max_len, truncation=trucation, padding=padding)\n",
    "    \n",
    "encodings = construct_encodings(x, tkzr, max_len=MAX_LEN)\n",
    "\n",
    "#The first stage of preprocessing is done! The second stage is converting our encodings and y (which holds the classes of the reviews) into a Tensorflow Dataset object. Below is a function to do this:\n",
    "def construct_tfdataset(encodings, y=None):\n",
    "    if y:\n",
    "        return tf.data.Dataset.from_tensor_slices((dict(encodings),y))\n",
    "    else:\n",
    "        # this case is used when making predictions on unseen samples after training\n",
    "        return tf.data.Dataset.from_tensor_slices(dict(encodings))\n",
    "    \n",
    "tfdataset = construct_tfdataset(encodings, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third and final preprocessing step is to create training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SPLIT = 0.2\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "train_size = int(len(x) * (1-TEST_SPLIT))\n",
    "\n",
    "tfdataset = tfdataset.shuffle(len(x))\n",
    "tfdataset_train = tfdataset.take(train_size)\n",
    "tfdataset_test = tfdataset.skip(train_size)\n",
    "\n",
    "tfdataset_train = tfdataset_train.batch(BATCH_SIZE)\n",
    "tfdataset_test = tfdataset_test.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 2\n",
    "\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "optimizer = optimizers.Adam(learning_rate=3e-5)\n",
    "loss = losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "model.fit(tfdataset_train, batch_size=BATCH_SIZE, epochs=N_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Testing the model\n",
    "\n",
    "Now we can use our test set to evaluate the performance of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks = model.evaluate(tfdataset_test, return_dict=True, batch_size=BATCH_SIZE)\n",
    "print(benchmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Using the fine-tuned model to predict new samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_predictor(model, model_name, max_len):\n",
    "    tkzr = DistilBertTokenizer.from_pretrained(model_name)\n",
    "    def predict_proba(text):\n",
    "        x = [text]\n",
    "\n",
    "        encodings = construct_encodings(x, tkzr, max_len=max_len)\n",
    "        tfdataset = construct_tfdataset(encodings)\n",
    "        tfdataset = tfdataset.batch(1)\n",
    "\n",
    "        preds = model.predict(tfdataset)\n",
    "        preds = activations.softmax(tf.convert_to_tensor(preds)).numpy()\n",
    "        return preds[0][0]\n",
    "    \n",
    "    return predict_proba\n",
    "\n",
    "clf = create_predictor(model, MODEL_NAME, MAX_LEN)\n",
    "print(clf('this restaurant has horrible food'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Saving and loading the model for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./model/clf')\n",
    "with open('./model/info.pkl', 'wb') as f:\n",
    "    pickle.dump((MODEL_NAME, MAX_LEN), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = TFDistilBertForSequenceClassification.from_pretrained('./model/clf')\n",
    "model_name, max_len = pickle.load(open('./model/info.pkl', 'rb'))\n",
    "\n",
    "clf = create_predictor(new_model, model_name, max_len)\n",
    "print('Sentiment [pos, neg]: ',clf('this restaurant has poor ambiance.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
